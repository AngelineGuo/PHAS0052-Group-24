{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the web-server NSitePred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio.PDB import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDB files to dataframes, including missing residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1d4x', '1j09', '1mb9', '1xdn', '1xdp', '1z0s', '2aqx', '2j9c', '2py7', '3amt']\n"
     ]
    }
   ],
   "source": [
    "# Function to parse PDB file and convert to DataFrame\n",
    "def pdb_to_dataframe(pdb_file):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('structure', pdb_file)\n",
    "    data = []\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    data.append([atom.serial_number, atom.name, residue.resname, chain.id, residue.id[1], \n",
    "                                 atom.coord[0], atom.coord[1], atom.coord[2], atom.occupancy,\n",
    "                                 atom.bfactor, atom.element])\n",
    "    columns = ['Atom_ID', 'Atom_Name', 'Residue_Name', 'Chain_ID', 'Residue_ID', \n",
    "               'X', 'Y', 'Z', 'Occupancy', 'Bfactor', 'Element']\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Directory containing the PDB files\n",
    "pdb_directory = '../Files/pdb'\n",
    "# Directory containing the true binding CSV files\n",
    "true_binding_directory = '../Files/true_bindings'\n",
    "\n",
    "# Dictionary to store dataframes\n",
    "pdb_dataframes = {}\n",
    "\n",
    "# Mapping of three-letter amino acid codes to one-letter codes\n",
    "three_to_one = {\n",
    "    'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E', 'PHE': 'F',\n",
    "    'GLY': 'G', 'HIS': 'H', 'ILE': 'I', 'LYS': 'K', 'LEU': 'L',\n",
    "    'MET': 'M', 'ASN': 'N', 'PRO': 'P', 'GLN': 'Q', 'ARG': 'R',\n",
    "    'SER': 'S', 'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'\n",
    "}\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for pdb_file in os.listdir(pdb_directory):\n",
    "    if not pdb_file.endswith('.pdb'):\n",
    "        continue\n",
    "    pdb_id, chain = pdb_file.split(\"_\")[0], pdb_file.split(\"_\")[1].split(\".\")[0]\n",
    "    \n",
    "    # Read the corresponding true binding CSV file\n",
    "    true_binding_file = os.path.join(true_binding_directory, f\"{pdb_id}_true.csv\")\n",
    "    true_binding_df = pd.read_csv(true_binding_file)\n",
    "    \n",
    "    # Skip this iteration if there is no ATP binding\n",
    "    if true_binding_df.empty:\n",
    "        continue  \n",
    "    \n",
    "    # Create a set of tuples of (Residue_Name, Residue_ID) for the true binding residues\n",
    "    true_binding_set = set(zip(true_binding_df['Residue_Name'], true_binding_df['Residue_ID']))\n",
    "    \n",
    "    filepath = os.path.join(pdb_directory, pdb_file)\n",
    "    pdb_df = pdb_to_dataframe(filepath)\n",
    "    \n",
    "    \n",
    "    # Filter rows for CA atoms\n",
    "    pdb_df = pdb_df[pdb_df['Atom_Name'] == 'CA']\n",
    "    # Filter rows for specific chain\n",
    "    pdb_df = pdb_df[pdb_df['Chain_ID'] == chain]\n",
    "    # Keep only specific columns\n",
    "    pdb_df = pdb_df[['Residue_Name', 'Residue_ID']]\n",
    "    \n",
    "    # add missing residues to the dataframe \n",
    "    h = parse_pdb_header(filepath)\n",
    "    # Go through missing residues and add to list\n",
    "    for residue in h['missing_residues']:\n",
    "        if residue[\"chain\"] == chain:\n",
    "            pdb_df = pd.concat([pdb_df, \n",
    "                                pd.DataFrame.from_records([{\n",
    "                                    'Residue_Name': residue[\"res_name\"], \n",
    "                                    'Residue_ID': residue[\"ssseq\"]}])])\n",
    "            \n",
    "    # Map three-letter amino acid codes to one-letter codes\n",
    "    pdb_df['Residue_Name'] = pdb_df['Residue_Name'].map(three_to_one)\n",
    "    # Remove duplicate rows\n",
    "    pdb_df = pdb_df.drop_duplicates()\n",
    "    # Sort by Residue_ID\n",
    "    pdb_df = pdb_df.sort_values(by='Residue_ID')\n",
    "    # Add the 'binding' column\n",
    "    pdb_df['binding'] = pdb_df.apply(\n",
    "        lambda row: 1 if (row['Residue_Name'], row['Residue_ID']) in true_binding_set \n",
    "                        else 0, axis=1)\n",
    "    # Reset index\n",
    "    pdb_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    pdb_dataframes[pdb_id] = pdb_df\n",
    "\n",
    "# Display the first 5 keys to show the imported proteins\n",
    "\n",
    "print(list(pdb_dataframes.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSite predictions to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1d4x', '1j09', '1mb9', '1xdn', '1xdp', '1z0s', '2aqx', '2j9c', '2py7', '3amt']\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the NSite predictions\n",
    "# directory = '../Files/NSite_predictions'\n",
    "directory = '../Bowen/NSitePred results'\n",
    "\n",
    "# Dictionary to store dataframes\n",
    "ns_predictions = {}\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for nsite_input_file in os.listdir(directory):\n",
    "    pdb_id = nsite_input_file.split(\"_\")[0]\n",
    "    if not nsite_input_file.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    filepath = os.path.join(directory, nsite_input_file)\n",
    "    nsite_df = pd.read_csv(filepath)\n",
    "    # Filter for relevant information\n",
    "    nsite_df = nsite_df[['AA', 'ATP binding res.', ' ATP prob.']]\n",
    "    nsite_df.rename(columns={'AA': 'Residue_Name',\n",
    "                                'ATP binding res.': 'binding',\n",
    "                                ' ATP prob.': 'probability'}, inplace=True)\n",
    "    # Map binding column to 0 and 1\n",
    "    nsite_df['binding'] = nsite_df['binding'].map({'B': 1, 'N': 0})\n",
    "    \n",
    "    # Get the starting Residue_ID from the corresponding PDB dataframe\n",
    "    if pdb_id in pdb_dataframes:\n",
    "        start_residue_id = pdb_dataframes[pdb_id]['Residue_ID'].iloc[0]\n",
    "        nsite_df.insert(1, 'Residue_ID', range(start_residue_id, start_residue_id + len(nsite_df)))\n",
    "        ns_predictions[pdb_id] = nsite_df\n",
    "\n",
    "# Display the keys of the dictionary to show the imported dataframes\n",
    "print(list(ns_predictions.keys())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of NSitePred web server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {} # Dict of merged dataframes\n",
    "\n",
    "for pdb_id, pdb_df in pdb_dataframes.items():\n",
    "    if pdb_id not in ns_predictions:\n",
    "        continue\n",
    "    nsite_df = ns_predictions[pdb_id]    \n",
    "        \n",
    "    # SQL join on Residue_ID\n",
    "    result = pd.merge(pdb_df, nsite_df, on='Residue_ID', how='inner', suffixes=('_true', '_pred'))\n",
    "    result.dropna(inplace=True)\n",
    "    results[pdb_id]= result \n",
    "    \n",
    "    # Print the pdb file where the residues do not match at some Residue_ID\n",
    "    # This is for debugging purposes\n",
    "    mismatch = result[result['Residue_Name_true'] != result['Residue_Name_pred']]\n",
    "    if not mismatch.empty:\n",
    "        print(pdb_id)\n",
    "        print(mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of metrics for the NSitePred web server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = $ \\frac{TP + TN}{TP + TN + FP + FN} $ \\\n",
    "\\\n",
    "Precision = $ \\frac{TP}{TP + FP} $ \\\n",
    "\\\n",
    "Recall = $ \\frac{TP}{TP + FN} $ \\\n",
    "\\\n",
    "F1 = $ 2\\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PDB_ID  TP  FP   TN  FN  Accuracy  Precision  Recall  F1-Score    AUC\n",
      "0   1d4x  14  11  343   7     0.952      0.560   0.667     0.609  0.913\n",
      "1   1j09  12   0  454   2     0.996      1.000   0.857     0.923  0.961\n",
      "2   1mb9  17   2  492   2     0.992      0.895   0.895     0.895  0.955\n",
      "3   1xdn  14   0  259   1     0.996      1.000   0.933     0.966  1.000\n",
      "4   1xdp  12   2  671   2     0.994      0.857   0.857     0.857  0.924\n"
     ]
    }
   ],
   "source": [
    "# Calculate the metrics for each PDB file\n",
    "results_directory = '../Files/results'\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "# Create a df to store the results\n",
    "results_df = pd.DataFrame(columns=['PDB_ID', 'TP', 'FP', 'TN', 'FN', \n",
    "                                   'Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC'])\n",
    "\n",
    "for pdb_id, result in results.items():\n",
    "    \n",
    "    # Compute the number of true positives, false positives, true negatives, and false negatives\n",
    "    result['TP'] = ((result['binding_true'] == 1) & (result['binding_pred'] == 1)).astype(int)\n",
    "    result['FP'] = ((result['binding_true'] == 0) & (result['binding_pred'] == 1)).astype(int)\n",
    "    result['TN'] = ((result['binding_true'] == 0) & (result['binding_pred'] == 0)).astype(int)\n",
    "    result['FN'] = ((result['binding_true'] == 1) & (result['binding_pred'] == 0)).astype(int)\n",
    "    \n",
    "    # Compute Metrics\n",
    "    accuracy = accuracy_score(result[\"binding_true\"], result[\"binding_pred\"])\n",
    "    precision = precision_score(result[\"binding_true\"], result[\"binding_pred\"])\n",
    "    recall = recall_score(result[\"binding_true\"], result[\"binding_pred\"])\n",
    "    f1 = f1_score(result[\"binding_true\"], result[\"binding_pred\"])\n",
    "    auc = roc_auc_score(result[\"binding_true\"], result[\"probability\"])\n",
    "    if np.isnan(auc):\n",
    "        auc = 0.0\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    metrics = {\n",
    "        'PDB_ID': pdb_id,\n",
    "        'TP': int(result['TP'].sum()),\n",
    "        'FP': int(result['FP'].sum()),\n",
    "        'TN': int(result['TN'].sum()),\n",
    "        'FN': int(result['FN'].sum()),\n",
    "        'Accuracy': round(float(accuracy), 3),\n",
    "        'Precision': round(float(precision), 3),\n",
    "        'Recall': round(float(recall), 3),\n",
    "        'F1-Score': round(float(f1), 3),\n",
    "        'AUC': round(float(auc), 3)\n",
    "    }\n",
    "    \n",
    "    # Add metrics as a new row to the dataframe\n",
    "    new_row = pd.DataFrame([metrics])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "print(results_df.head())   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
